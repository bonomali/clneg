{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Notebook for Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import nltk\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from jsonrpclib.jsonrpc import ServerProxy\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "data_dir = '../data/'\n",
    "tregex_dir = './stanford-tregex-2018-02-27/'\n",
    "ctakes_folder = './ctakes/'\n",
    "\n",
    "# can be extended to batch processing if needed (feed a list of filenames)\n",
    "#filenames = ['dev.txt']\n",
    "#filenames = ['3.txt']\n",
    "filenames = ['test_ready.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_list = pd.read_csv(data_dir + 'multilingual_lexicon-en-de-fr-sv.csv', sep=',', header=0)[['ITEM', 'CATEGORY', 'EN (SV) ACTION']]\n",
    "neg_list = neg_list[neg_list['CATEGORY'].isin(['definiteNegatedExistence', 'probableNegatedExistence', 'pseudoNegation'])]\n",
    "neg_list['NEG'] = ''\n",
    "neg_list['FIRST_TOKEN'] = ''\n",
    "neg_list['FIRST_POS'] = ''\n",
    "neg_list['LAST_TOKEN'] = ''\n",
    "neg_list['LAST_POS'] = ''\n",
    "for idx in neg_list.index:\n",
    "    if neg_list['CATEGORY'][idx] == 'definiteNegatedExistence' and neg_list['EN (SV) ACTION'][idx] == 'forward': \n",
    "        neg_list['NEG'][idx] = 'PREN'\n",
    "    if neg_list['CATEGORY'][idx] == 'definiteNegatedExistence' and neg_list['EN (SV) ACTION'][idx] == 'backward': \n",
    "        neg_list['NEG'][idx] = 'POST'\n",
    "    if neg_list['CATEGORY'][idx] == 'definiteNegatedExistence' and neg_list['EN (SV) ACTION'][idx] == 'bidirectional': \n",
    "        neg_list['NEG'][idx] = 'POST'\n",
    "    if neg_list['CATEGORY'][idx] == 'probableNegatedExistence' and neg_list['EN (SV) ACTION'][idx] == 'forward':\n",
    "        neg_list['NEG'][idx] = 'PREP'\n",
    "    if neg_list['CATEGORY'][idx] == 'probableNegatedExistence' and neg_list['EN (SV) ACTION'][idx] == 'backward': \n",
    "        neg_list['NEG'][idx] = 'POSP'\n",
    "    if neg_list['CATEGORY'][idx] == 'probableNegatedExistence' and neg_list['EN (SV) ACTION'][idx] == 'bidirectional': \n",
    "        neg_list['NEG'][idx] = 'POSP'\n",
    "    if neg_list['CATEGORY'][idx] == 'pseudoNegation': \n",
    "        neg_list['NEG'][idx] = 'PSEU'\n",
    "    neg_list['FIRST_TOKEN'][idx] = neg_list['ITEM'][idx].split()[0]\n",
    "    neg_list['FIRST_POS'][idx] = nltk.pos_tag(nltk.word_tokenize(neg_list['FIRST_TOKEN'][idx]))[0][1]\n",
    "    neg_list['LAST_TOKEN'][idx] = neg_list['ITEM'][idx].split()[len(neg_list['ITEM'][idx].split())-1]\n",
    "    neg_list['LAST_POS'][idx] = nltk.pos_tag(nltk.word_tokenize(neg_list['LAST_TOKEN'][idx]))[0][1]\n",
    "\n",
    "neg = neg_list['ITEM'].values\n",
    "neg_list.head()\n",
    "neg_list.to_csv(data_dir + 'neg_list.txt', sep='\\t', index=False, quoting=False)\n",
    "\n",
    "neg_term = [' ' + item + ' ' for item in neg]\n",
    "neg_term.extend(item + ' ' for item in neg)\n",
    "\n",
    "# use the labeled list (annotated 'type')\n",
    "neg_list = pd.read_csv(data_dir + 'neg_list_complete.txt', sep='\\t', header=0)\n",
    "neg = neg_list['ITEM'].values\n",
    "neg_term = [' ' + item + ' ' for item in neg]\n",
    "neg_term.extend(item + ' ' for item in neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section and senetence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_names = ['Allergies', 'Chief Complaint', 'Major Surgical or Invasive Procedure', 'History of Present Illness',\n",
    "                'Past Medical History', 'Social History', 'Family History', 'Brief Hospital Course', \n",
    "                'Medications on Admission', 'Discharge Medications', 'Discharge Diagnosis', 'Discharge Condition', \n",
    "                 'Discharge Instructions']\n",
    "section_dict ={\n",
    " 'Allergies': ['allergy'],\n",
    " 'Brief Hospital Course': ['hospital', 'course'],\n",
    " 'Chief Complaint': ['chief', 'complaint'],\n",
    " 'Discharge Condition': ['discharge', 'condition'],\n",
    " 'Discharge Diagnosis': ['discharge', 'diagnosis'],\n",
    " 'Discharge Instructions': ['discharge', 'instruction'],\n",
    " 'Discharge Medications': ['discharge', 'medication'],\n",
    " 'Family History': ['family', 'history'],\n",
    " 'History of Present Illness': ['history', 'present', 'illness'],\n",
    " 'Major Surgical or Invasive Procedure': ['major',\n",
    "  'surgical',\n",
    "  'invasive',\n",
    "  'procedure'],\n",
    " 'Medications on Admission': ['medication', 'admission'],\n",
    " 'Past Medical History': ['medical', 'history'],\n",
    " 'Social History': ['social', 'history']}\n",
    "\n",
    "other_section_names = ['Followup Instructions', 'Physical Exam', 'Pertinent Results', 'Facility', 'Discharge Disposition']\n",
    "other_section_dict = {\n",
    " 'Discharge Disposition': ['discharge', 'disposition'],\n",
    " 'Facility': ['facility'],\n",
    " 'Followup Instructions': ['followup', 'instruction'],\n",
    " 'Pertinent Results': ['pertinent', 'result'],\n",
    " 'Physical Exam': ['physical', 'exam']}\n",
    "\n",
    "all_section_dict = {}\n",
    "all_section_dict.update(section_dict)\n",
    "all_section_dict.update(other_section_dict)\n",
    "\n",
    "section_names_list = list(section_dict.keys())\n",
    "\n",
    "section_to_parse = ['History of Present Illness', 'Brief Hospital Course', 'Discharge Instructions']\n",
    "section_not_to_parse = [item for item in section_names_list if item not in section_to_parse] + ['None']\n",
    "\n",
    "hard_section_list = ['History of Present Illness', 'Past Medical History', 'Brief Hospital Course', 'Discharge Diagnosis', 'Discharge Instructions']\n",
    "easy_section_list = [item for item in section_names_list if item not in hard_section_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_section_name(name, section_dict, nlp_parser):\n",
    "    output = nlp_parser.annotate(name.lower(), properties={\n",
    "                                              'annotators': 'lemma',\n",
    "                                              'outputFormat': 'json',\n",
    "                                              'threads': '4',\n",
    "                                              'tokenize.options': 'normalizeParentheses=false, normalizeOtherBrackets=false'\n",
    "                                              })\n",
    "    try:\n",
    "        name_lemma = set([[str(token['lemma']) for token in sent['tokens']] for sent in output['sentences']][0])\n",
    "    except:\n",
    "        return 'None'\n",
    "    else:\n",
    "        for section_name, section_name_lemma in section_dict.items():\n",
    "            if all([item in name_lemma for item in section_name_lemma]):\n",
    "                return section_name\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(1):\n",
    "    sections = {}\n",
    "    sections['None'] = []\n",
    "    with open(os.path.join(data_dir, filenames[idx]), 'r') as f:\n",
    "        for _ in range(3): next(f)\n",
    "        lines_buffer = []\n",
    "        previous_section_name = 'None'\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if line.lower() == 'attending:':\n",
    "                    continue\n",
    "                lines_buffer.append(line)\n",
    "            else:\n",
    "                if lines_buffer:\n",
    "                    lines_buffer_head = lines_buffer[0]\n",
    "                    if ':' in lines_buffer_head:\n",
    "                        section_name = lines_buffer_head.split(':')[0]\n",
    "                        matched_section_name = match_section_name(section_name, all_section_dict, nlp)\n",
    "                        if matched_section_name != 'None':\n",
    "                            previous_section_name = matched_section_name\n",
    "                            if len(lines_buffer_head.split(':')[1:]) > 1:\n",
    "                                sections[matched_section_name] = [' '.join(lines_buffer_head.split(':')[1:])] + lines_buffer[1:]\n",
    "                            else:\n",
    "                                sections[matched_section_name] = lines_buffer[1:]\n",
    "                            lines_buffer = []\n",
    "                            continue\n",
    "\n",
    "                    sections[previous_section_name] = sections.get(previous_section_name, None) + lines_buffer\n",
    "                lines_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for section_name in section_to_parse:\n",
    "    if section_name in sections:\n",
    "        text = ' '.join(sections[section_name])\n",
    "        output = nlp.annotate(text, properties={\n",
    "                                          'annotators': 'ssplit',\n",
    "                                          'outputFormat': 'json',\n",
    "                                          'threads': '4',\n",
    "                                          'tokenize.options': 'normalizeParentheses=false, normalizeOtherBrackets=false'\n",
    "                                          })\n",
    "        try:\n",
    "            sents = [[str(token['word']) for token in sent['tokens']] for sent in output['sentences']]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        else:\n",
    "            sections[section_name] = [' '.join(sent) for sent in sents if sent != ['.']]\n",
    "\n",
    "for section_name in section_not_to_parse:\n",
    "    if section_name in sections:\n",
    "        new_section_content = []\n",
    "        for text in sections[section_name]:\n",
    "            output = nlp.annotate(text, properties={\n",
    "                                              'annotators': 'ssplit',\n",
    "                                              'outputFormat': 'json',\n",
    "                                              'threads': '4',\n",
    "                                              'tokenize.options': 'normalizeParentheses=false, normalizeOtherBrackets=false'\n",
    "                                              })\n",
    "            try:\n",
    "                sents = [[str(token['word']) for token in sent['tokens']] for sent in output['sentences']]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            else:\n",
    "                new_section_content.append(' '.join([' '.join(sent) for sent in sents if sent != ['.']]))\n",
    "        sections[section_name] = new_section_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_dir + 'tmp', 'w') as f:\n",
    "    for section_name in hard_section_list:\n",
    "        # add section head tag\n",
    "        f.write('\\n\\n\\n\\n[SECTION-{}-START]'.format(section_name))\n",
    "        if section_name in sections:\n",
    "            for item in sections[section_name]:\n",
    "                # tag negated or affirmed based on string matching --- negation term list\n",
    "                # add one space to prevent loss of 'no ', 'not ', ... etc.\n",
    "                if any(substring in ' ' + item for substring in neg_term):\n",
    "                    f.write('\\n\\n\\n\\n' + item + '\\t [NEGATED]')\n",
    "                else:\n",
    "                    f.write('\\n\\n\\n\\n' + item + '\\t [AFFIRMED]')\n",
    "        # add section end tag\n",
    "        f.write('\\n\\n\\n\\n[SECTION-{}-END]'.format(section_name)) # this file for concept extraction and sentence parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Concept extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cui_spans(xml_filename):\n",
    "    tree = etree.parse(xml_filename)\n",
    "    textsems = tree.xpath('*[@_ref_ontologyConceptArr]')\n",
    "    span = lambda e: (int(e.get('begin')), int(e.get('end')))\n",
    "    ref_to_span = {e.get('_ref_ontologyConceptArr'): span(e) for e in textsems}\n",
    "    fsarrays = tree.xpath('uima.cas.FSArray')\n",
    "    id_to_ref = {e.text: fs.get('_id') for fs in fsarrays for e in fs}\n",
    "    umlsconcepts = tree.xpath('org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "    cui_ids = [(c.get('cui'), c.get('tui'), c.get('preferredText'), c.get('_id')) for c in umlsconcepts]\n",
    "    id_to_span = lambda _id: ref_to_span[id_to_ref[_id]]\n",
    "    cui_spans = [(cui, tui, pt, id_to_span(_id)) for cui, tui, pt, _id in cui_ids]    \n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [cs for cs in cui_spans if not (cs in seen or seen_add(cs))]\n",
    "\n",
    "\n",
    "def extract_cuis(xml_filename):\n",
    "    cui_spans = get_cui_spans(xml_filename)\n",
    "    cui_spans.sort(key=lambda cs: cs[3])\n",
    "    row_id = os.path.basename(xml_filename).split('.')[0]\n",
    "    txt = etree.parse(xml_filename).xpath('uima.cas.Sofa')[0].get('sofaString')\n",
    "    return [(row_id, str(cs[3][0]), str(cs[3][1]), cs[0], cs[1], txt[(cs[3][0]):(cs[3][1])], cs[2]) for cs in cui_spans]\n",
    "\n",
    "# keep: 047, 046, 033, 184, 061, 048, 131\n",
    "# discard: 029, 034, 197, 121, 023, 059, 060, 195, 109, 022, 122, \n",
    "\n",
    "d = {\n",
    "  \"ddx\": [\"T047\", \"T191\"], # disease/disorder/syndrome \n",
    "  \"ssx\": [\"T033\", \"T040\", \"T046\", \"T048\", \"T049\", \"T184\"], # symptoms/signs\n",
    "  \"med\": [\"T116\", \"T123\", \"T126\", \"T131\"], # medications\n",
    "  \"dxp\": [], # diagnostic proc\n",
    "  \"txp\": [\"T061\"], # therapeutic proc\n",
    "  \"lab\": [], # labs\n",
    "  \"ana\": [\"T017\", \"T024\", \"T025\"], # anatomy\n",
    "}\n",
    "\n",
    "tui_list = []\n",
    "for k, v in d.items():\n",
    "    tui_list.extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def execute(command):\n",
    "#     process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "#     while True:\n",
    "#         nextline = process.stdout.readline()\n",
    "#         if nextline == '' and process.poll() is not None:\n",
    "#             break\n",
    "#         sys.stdout.write(nextline)\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "#     output = process.communicate()[0]\n",
    "#     exitCode = process.returncode\n",
    "\n",
    "#     if (exitCode == 0):\n",
    "#         return output\n",
    "#     else:\n",
    "#         raise ProcessException(command, exitCode, output)\n",
    "        \n",
    "os.system('find . -name \".DS_Store\" -type f -delete -print; ')\n",
    "os.system('cp ' + data_dir + 'tmp ' + ctakes_folder + 'note_input/')\n",
    "os.system('sh ' + ctakes_folder + 'bin/pipeline.sh')\n",
    "#(output, err) = p.communicate()\n",
    "os.system('rm ' + ctakes_folder + 'note_input/tmp')\n",
    "os.system('mv ' + ctakes_folder + 'note_output/tmp.xml '+ data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #subprocess.check_output(['bash', '-c', 'find . -name \"' + data_dir + 'tmp\" -exec cp {} ' + ctakes_folder + 'note_input/ \\;'])\n",
    "# subprocess.Popen('find . -name \".DS_Store\" -type f -delete -print', stdout=subprocess.PIPE, shell=True)\n",
    "# subprocess.Popen('cp ' + data_dir + 'tmp ' + ctakes_folder + 'note_input/', stdout=subprocess.PIPE, shell=True)\n",
    "# p = subprocess.Popen(\"sh \" + ctakes_folder + \"bin/pipeline.sh\", stdout=subprocess.PIPE, shell=True)\n",
    "# (output, err) = p.communicate()\n",
    "# subprocess.check_output(['bash', '-c', 'find . -name \"' + ctakes_folder + 'note_output/tmp.xml\" -exec cp {} ../data/ \\;'])\n",
    "# subprocess.Popen('rm ' + ctakes_folder + 'note_input/tmp', stdout=subprocess.PIPE, shell=True)\n",
    "# subprocess.Popen('mv ' + ctakes_folder + 'note_output/tmp.xml '+ data_dir, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#os.system('rm tmp')\n",
    "d = [e for e in extract_cuis(data_dir + 'tmp.xml')]\n",
    "df = pd.DataFrame(d, columns=['fname', 'start', 'end', 'cui', 'tui', 'original', 'preferred'])\n",
    "df = df[df['tui'].isin(tui_list)]\n",
    "\n",
    "with open(data_dir + 'tmp', 'r') as f:\n",
    "    doc = f.read()\n",
    "\n",
    "sec_dict = {}\n",
    "for sec_head in hard_section_list:\n",
    "    sec_dict[sec_head] = (doc.index('[SECTION-' + sec_head + '-START]') + len('[SECTION-' + sec_head + '-START]'), \\\n",
    "                          doc.index('[SECTION-' + sec_head + '-END]'))\n",
    "sec_dict\n",
    "\n",
    "df['section'] = ''\n",
    "for idx in df.index:\n",
    "    for k, v in sec_dict.iteritems():\n",
    "        if int(df['start'][idx]) > v[0] and int(df['end'][idx]) < v[1]:\n",
    "            df['section'][idx] = str(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = [e for e in extract_cuis(data_dir + 'tmp.xml')]\n",
    "# df = pd.DataFrame(d, columns=['fname', 'start', 'end', 'cui', 'tui', 'original', 'preferred'])\n",
    "# with open(data_dir + 'tmp', 'r') as f:\n",
    "#     doc = f.read()\n",
    "\n",
    "# sec_dict = {}\n",
    "# for sec_head in hard_section_list:\n",
    "#     sec_dict[sec_head] = (doc.index('[SECTION-' + sec_head + '-START]') + len('[SECTION-' + sec_head + '-START]'), \\\n",
    "#                           doc.index('[SECTION-' + sec_head + '-END]'))\n",
    "# sec_dict\n",
    "\n",
    "# df['section'] = ''\n",
    "# for idx in df.index:\n",
    "#     for k, v in sec_dict.iteritems():\n",
    "#         if int(df['start'][idx]) > v[0] and int(df['end'][idx]) < v[1]:\n",
    "#             df['section'][idx] = str(k)\n",
    "# df[df.section != \"\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_neg_start = [s.start() for s in re.finditer('\\\\n\\\\n\\\\n\\\\n.*\\\\t \\[NEGATED\\]', doc)]\n",
    "s_neg_end = [s.start() for s in re.finditer('\\[NEGATED\\]', doc)]\n",
    "s_neg = zip(s_neg_start, s_neg_end) # range of negation in sentence level\n",
    "neg_range_list = [range(r[0], r[1]) for r in s_neg]\n",
    "#neg_range_list = [y for x in neg_range_list for y in x]\n",
    "\n",
    "df['negation'] = 0\n",
    "df['sent_id'] = 0\n",
    "df['sent_loc'] = 0\n",
    "for idx in df.index:\n",
    "    for i, nl in enumerate(neg_range_list):\n",
    "        if int(df['start'][idx]) in nl:\n",
    "            df['negation'][idx] = 0\n",
    "            df['sent_id'][idx] = i + 1 # sent_id from 1\n",
    "            df['sent_loc'][idx] = int(df['start'][idx]) - nl[0] + 1 # sent_loc also start from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df[df.sent_id != 0]\n",
    "df0 = df[df.sent_id == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class OpenNLP:\n",
    "    def __init__(self, host='localhost', port=8080):\n",
    "        uri = \"http://%s:%d\" % (host, port)\n",
    "        self.server = ServerProxy(uri)\n",
    "\n",
    "    def parse(self, text):\n",
    "        return self.server.parse(text)\n",
    "\n",
    "nlp = OpenNLP()\n",
    "\n",
    "\n",
    "# preparing sentence for parsing\n",
    "l = []\n",
    "sl = []\n",
    "with open(data_dir + 'tmp') as fr:\n",
    "    for sent in fr:\n",
    "        if sent.endswith('[NEGATED]\\n') or sent == '\\n':\n",
    "            l.append(sent)\n",
    "        if sent.endswith('[NEGATED]\\n'):\n",
    "            sl.append(sent)\n",
    "\n",
    "# # opennlp parsing\n",
    "# print '\\n--- parse full sentence ---\\n'\n",
    "# tree_list = []\n",
    "# with open(data_dir + 'tmp_tree', 'w') as fw:        \n",
    "#     for i, s in enumerate(l):\n",
    "#         t = (nlp.parse(s.replace('[NEGATED]', '')))\n",
    "#         if t != '':\n",
    "# #             print s\n",
    "# #             print i, t\n",
    "#             fw.write(t + '\\n')\n",
    "#             tree_list.append(t)\n",
    "        \n",
    "# print len(sl)\n",
    "# print len(tree_list)           \n",
    "    \n",
    "\n",
    "# remove before/after words!\n",
    "# neg_front = [i + ' ' for i in neg_list[neg_list['EN (SV) ACTION'] == 'forward']['ITEM'].tolist()]\n",
    "# neg_back = [' ' + i for i in neg_list[neg_list['EN (SV) ACTION'] == 'backward']['ITEM'].tolist()]\n",
    "\n",
    "ll = []\n",
    "for ss in l:\n",
    "    s = ''\n",
    "    flag = ''\n",
    "    for nw in sorted(neg_list['ITEM'].tolist(), key=len, reverse=True):\n",
    "        if nw in neg_list[neg_list['EN (SV) ACTION'] == 'forward']['ITEM'].tolist():\n",
    "            try:\n",
    "                s = ss[ss.index(nw):]\n",
    "                flag = 'f'\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                s = ss[:(ss.index(nw)+len(nw))]\n",
    "                flag = 'b'\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    ll.append(s)\n",
    "    \n",
    "    \n",
    "tree_list = []\n",
    "while len(sl) != len(tree_list): # run until opennlp can parse with correct number of sentences. bug???\n",
    "    # opennlp parsing the neg tree\n",
    "    print('\\n--- parse negated part of the sentence ---\\n')\n",
    "    tree_list = []\n",
    "    with open(data_dir + 'tmp_neg_tree', 'w') as fw:\n",
    "        for i, s in enumerate(ll):\n",
    "            t = (nlp.parse(s.replace('[NEGATED]', '')))\n",
    "            if t != '':\n",
    "#                 print s\n",
    "#                 print i, t\n",
    "                fw.write(t + '\\n')\n",
    "                tree_list.append(t)\n",
    "    print len(sl)\n",
    "    print len(tree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using stanford corenlp parsing too slow\n",
    "import requests\n",
    "def extract_subtree(text, tregex):\n",
    "    r = requests.post(url=\"http://localhost:9000/tregex\", \n",
    "                      data=text, \n",
    "                      params={\"pattern\": tregex})\n",
    "    js = r.json()\n",
    "    if js['sentences'][0] and '0' in js['sentences'][0] and 'namedNodes' in js['sentences'][0]['0']:\n",
    "        return js['sentences'][0]['0']['namedNodes']\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_subtree_treefile(f, tregex):\n",
    "    t = subprocess.Popen(tregex_dir + 'tregex.sh ' + tregex + ' ' + f , stdout=subprocess.PIPE, shell=True)\n",
    "    p = subprocess.Popen(tregex_dir + 'tregex.sh ' + tregex + ' ' + f + ' -t', stdout=subprocess.PIPE, shell=True)\n",
    "    (tree, err) = t.communicate()\n",
    "    (output, err) = p.communicate()\n",
    "    print(tree)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def tregex_tsurgeon(f, pos):\n",
    "    cmd = trts[pos][0] + '\\n\\n' + trts[pos][1].replace(',', '\\n')\n",
    "    with open('./stanford-tregex-2018-02-27/ts', 'w') as fw:     \n",
    "        fw.write(cmd)\n",
    "    t = subprocess.Popen('cd ' + tregex_dir + '; ./tsurgeon.sh -treeFile ../' + f + ' ts; cd ..', stdout=subprocess.PIPE, shell=True)\n",
    "    p = subprocess.Popen('cd ' + tregex_dir + '; ./tsurgeon.sh -treeFile ../' + f + ' ts -s; cd ..', stdout=subprocess.PIPE, shell=True)\n",
    "    (tree, err) = t.communicate()\n",
    "    (output, err) = p.communicate()\n",
    "    print('constituency tree: ' + output.replace('\\n', ''))\n",
    "    ts_out = re.sub('\\([A-Z]*\\$? |\\(-[A-Z]+- |\\)|\\)|\\(, |\\(. |\\n', '', output)\n",
    "    ts_out = re.sub('-LRB-', '(', ts_out)\n",
    "    ts_out = re.sub('-RRB-', ')', ts_out)\n",
    "    return ts_out, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trts = {}\n",
    "# no \"jvd|murmurs|deficits\" not work, pleural -> vbz?\n",
    "# trts['NP'] = ('NP=target << DT=neg <<, /no|without/ !> NP >> TOP=t >> S=s', \\\n",
    "#               'excise s target,delete neg')\n",
    "\n",
    "# if np with top node=S???\n",
    "# trts['NP'] = ('NP=target << DT=neg <<, /no|without/ !> NP >> TOP=t >> S=s', \\\n",
    "#               'excise s target,delete neg')\n",
    "trts['NP'] = ('NP=target << DT=neg <<, /no|without/ !> NP >> TOP=t', \\\n",
    "              'delete neg')\n",
    "# if np with top node=NP\n",
    "trts['NP-nS'] = ('NP=target <<, /DT|NN|RB/=neg <<, /no|without/ !> NP >> TOP=t', \\\n",
    "              'delete neg')\n",
    "\n",
    "\n",
    "# denies -> mis pos to nns\n",
    "trts['NP-denies'] = ('NP=target <<, /denies|deny|denied/=neg >> TOP=t', \\\n",
    "              'delete neg')\n",
    "\n",
    "# vp only\n",
    "trts['VP-A'] = ('VP=target << /VBZ|VBD|VB/=neg >> TOP=t', \\\n",
    "              'delete neg')\n",
    "trts['VP-CC'] = ('VP=target <<, /VBZ|VBD|VB/=neg < CC >> TOP=t', \\\n",
    "              'delete neg')\n",
    "# vp only, 'resolved', add that neg1 part to prevent jvd -> VP, rashes -> VP error pos tagging\n",
    "# trts['VP-P'] = ('NP=target <<, DT=neg1 <<, /no|negative|not/ $ VP=neg2 >> TOP=t >> S=s', \\\n",
    "#               'delete neg1')\n",
    "trts['VP-P'] = ('VP=vp <<- /free|negative|absent|ruled|out|doubtful|unlikely|excluded|resolved|given/=neg $ NP=head >> TOP=t >> S=s', \\\n",
    "              'excise s head')\n",
    "# this is post, ... is negative\n",
    "# trts['ADJP-P'] = ('VP=vp < ADJP <<- /negative/=neg $ NP=target >> TOP=t >> S=s', \\\n",
    "#                 'delete vp,excise s target')\n",
    "trts['ADJP-P'] = ('VP=vp <<- /free|negative|absent|ruled|out|doubtful|unlikely|excluded|resolved|given/=neg $ NP=head >> TOP=t >> S=s', \\\n",
    "                'excise s head')\n",
    "# this is ant, negative for ...\n",
    "# trts['ADJP-A'] = ('PP=head $ JJ=neg < NP=target >> TOP=t > ADJP=s', \\\n",
    "#                 'delete neg')\n",
    "trts['ADJP-A'] = ('PP=head $ /JJ|ADJP|NP/=neg <- NP=target >> TOP=t >> /S|NP/=s', \\\n",
    "                'excise s target')\n",
    "# not\n",
    "# trts['ADVP-P'] = ('VP=target <<, /VB*|MD/ $ RB=neg >> TOP=t >> S=s', \\\n",
    "#                 'excise s target')\n",
    "trts['ADVP-P'] = ('VP=head $ RB=neg <<, /VB*|MD/=be >> TOP=t >> S=s', \\\n",
    "                'delete head,delete neg')\n",
    "\n",
    "# trts['ADVP-A'] = ('VP=target <<, /VB*|MD/ $ RB=neg >> TOP=t >> S=s', \\\n",
    "#                 'excise s target')\n",
    "trts['ADVP-A'] = ('VP=head $ RB=neg <<, /VB*|MD/ >> TOP=t >> S=s', \\\n",
    "                'excise s head')\n",
    "trts['ADVP-A2'] = ('VP=head << RB=neg <<, /VB*|MD/ << /ADJP|VP/=target >> TOP=t >> S=s', \\\n",
    "                'excise s target')\n",
    "# remove sbar\n",
    "trts['ADVP-sbar'] = ('PP=head <<, /of|without/=neg > NP $ NP < NP=target >> TOP=t >> NP=st << SBAR=sbar', \\\n",
    "                'excise st target,delete sbar')\n",
    "trts['ADVP-advp'] = ('ADVP=advp', \\\n",
    "                'delete advp')\n",
    "trts['forced-sbar'] = ('SBAR=sbar', \\\n",
    "                'delete sbar')\n",
    "\n",
    "# remove RB\n",
    "trts['ADVP-RB'] = ('TOP=target <<, RB=neg', \\\n",
    "                'delete neg')\n",
    "\n",
    "# sob become this, so need to be after np and vp\n",
    "# trts['PP'] = ('PP=head <<, /of|without/=neg > NP $ NP < NP=target >> TOP=t >> NP=s', \\\n",
    "#               'excise s target')\n",
    "trts['PP'] = ('PP=head <<, IN=neg1 < NP=target >> TOP=t >> /S|NP|ADJP/=s $ /JJ|NP/=neg2', \\\n",
    "              'excise s target')\n",
    "trts['PP-2'] = ('PP=head << IN=neg <<, /of|without/ >> TOP=t', \\\n",
    "                'delete neg')\n",
    "\n",
    "trts['NP-CC'] = ('S=s < NP =head<< PP=target << DT=neg <<, /no|without/ < CC=but << but < S=rm < /\\.|\\,/=punct << SBAR=sbar !> NP > TOP=t', \n",
    "                 'delete neg,delete sbar,delete punct,delete but,delete rm')\n",
    "trts['NP-although'] = ('S=s < NP =head<< PP=target << DT=neg <<, /no|without/ << /although|but/ < /\\.|\\,/=punct << SBAR=sbar !> NP > TOP=t', \n",
    "                       'delete neg,delete sbar,delete punct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "RM_POS = ['NN', 'NNS', 'RB', 'NP', 'ADVP', 'IN']\n",
    "RM_CP = ['however', 'although', 'but']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "for i, t in enumerate(tree_list):\n",
    "    print('sent: ' + str(i))\n",
    "    print('original: ' + sl[i])\n",
    "    \n",
    "    # get negated part of the sentence\n",
    "    with open(data_dir + 'ntree_tmp', 'w') as fw:     \n",
    "        fw.write(t)\n",
    "    s = re.sub('\\([A-Z]*\\$? |\\(-[A-Z]+- |\\)|\\)|\\(, |\\(. ', '', t)\n",
    "    print('neg part: ' + s)\n",
    "    \n",
    "    # find what neg term is matched and use its neg type\n",
    "    try:\n",
    "        m = ''\n",
    "        for neg in [x for x in sorted(neg_list['ITEM'].tolist(), key=len, reverse=True)]:\n",
    "        #for neg in ['negative for']:\n",
    "            match = SequenceMatcher(None, s, neg).find_longest_match(0, len(s), 0, len(neg))\n",
    "            matched_string = s[match.a: match.a + match.size]\n",
    "            try: # if next char might be different, means partial match\n",
    "                if s[match.a + match.size + 1] == neg[match.b + match.size + 1] and \\\n",
    "                   s[match.a + match.size + 2] == neg[match.b + match.size + 2]:\n",
    "                    if (len(matched_string) > len(m)) and \\\n",
    "                        ((matched_string[0] == s[0] and matched_string[1] == s[1]) or \\\n",
    "                         (matched_string[len(matched_string)-1] == s[len(s)-1] and matched_string[len(matched_string)-2] == s[len(s)-2])): # either match from the beginning or laast\n",
    "                        m = matched_string \n",
    "                        matched_neg_item = neg[match.b: match.b + match.size]\n",
    "                        if matched_neg_item[len(matched_neg_item)-1] == ' ':\n",
    "                            matched_neg_item = matched_neg_item[0:len(matched_neg_item)-1]\n",
    "                else:\n",
    "                    continue\n",
    "            except: # if no next char, means full match\n",
    "                try:\n",
    "                    if (len(matched_string) > len(m)) and \\\n",
    "                        ((matched_string[0] == s[0] and matched_string[1] == s[1]) or \\\n",
    "                         (matched_string[len(matched_string)-1] == s[len(s)-1] and matched_string[len(matched_string)-2] == s[len(s)-2])): # either match from the beginning or laast\n",
    "                        m = matched_string \n",
    "                        matched_neg_item = neg[match.b: match.b + match.size]\n",
    "                        if matched_neg_item[len(matched_neg_item)-1] == ' ':\n",
    "                            matched_neg_item = matched_neg_item[0:len(matched_neg_item)-1]\n",
    "                except: # match only one char!? rare case\n",
    "                    if (len(matched_string) > len(m)) and \\\n",
    "                        (matched_string[0] == s[0]): # either match from the beginning or laast   \n",
    "                        m = matched_string\n",
    "                        matched_neg_item = neg[match.b: match.b + match.size]\n",
    "                        if matched_neg_item[len(matched_neg_item)-1] == ' ':\n",
    "                            matched_neg_item = matched_neg_item[0:len(matched_neg_item)-1]                    \n",
    "        print('negated term: ' + matched_neg_item)\n",
    "        \n",
    "        neg_type = neg_list[neg_list.ITEM == matched_neg_item]['TYPE'].values[0]\n",
    "        print('--- tregex/tsurgeon with negated type: ' + neg_type)\n",
    "\n",
    "        # run tregex/tsurgeon based on the selected neg type\n",
    "        ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', neg_type)\n",
    "\n",
    "        # deal with corner cases\n",
    "        if neg_type == 'NP' and ('that' in ts_out):\n",
    "            print('--- NP with that')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'NP-denies')\n",
    "        if neg_type == 'NP' and s == ts_out:\n",
    "            print('--- NP without S node')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'NP-nS')\n",
    "        \n",
    "        if neg_type == 'PP' and sum([item in neg_list['ITEM'].tolist() for item in ts_out.split()]) > 0:\n",
    "            print('--- NP without S node')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'NP-nS')\n",
    "            \n",
    "        if neg_type == 'VP-A' and s == ts_out:\n",
    "            print('--- VP-A remove denies')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'NP-denies')\n",
    "            \n",
    "        if neg_type == 'ADVP-A' and s == ts_out:\n",
    "            print('--- ADVP-A type 2')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'ADVP-A2')\n",
    "        if neg_type == 'ADVP-A' and s == ts_out:\n",
    "            print('--- ADVP-A remove SBAR')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'ADVP-sbar')\n",
    "        if neg_type == 'ADVP-A' and s == ts_out: # no longer\n",
    "            print('--- ADVP-A remove ADVP')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'ADVP-advp')\n",
    "        if neg_type == 'ADVP-A' and s == ts_out:\n",
    "            print('--- ADVP-A remove RB')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'ADVP-RB')\n",
    "        \n",
    "        if 'SBAR' in tree:\n",
    "            print('--- forced remove SBAR')\n",
    "            ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'forced-sbar')\n",
    "            \n",
    "#         if sum([item in neg_list['ITEM'].tolist() for item in ts_out.split()]) > 0:\n",
    "#             print('--- remove neg terms if exists')\n",
    "#             ts_out = ' '.join(ts_out.split()[1:])\n",
    "            \n",
    "        if sum([item in RM_POS for item in ts_out.split()]) > 0:\n",
    "            print('--- remove POS')\n",
    "            ts_out = ' '.join(ts_out.split()[1:])\n",
    "            \n",
    "        if sum([item in RM_CP for item in ts_out.split()]) > 0:\n",
    "            print('--- remove CP')\n",
    "            for cp in RM_CP:\n",
    "                try:\n",
    "                    cp_loc = ts_out.split().index(cp)\n",
    "                except:\n",
    "                    continue\n",
    "            ts_out = ' '.join(ts_out.split()[:cp_loc])\n",
    "            \n",
    "        if ts_out.split()[0] in neg_list['ITEM'].tolist() + stopwords:\n",
    "            print('--- remove first token f if f in negated list or stopword list')\n",
    "            ts_out = ' '.join(ts_out.split()[1:])\n",
    "\n",
    "#         if neg_type == 'VP-A' and len(ts_out) < 2:\n",
    "#             print('--- VP-A CC')\n",
    "#             ts_out, tree = tregex_tsurgeon(data_dir + 'ntree_tmp', 'VP-CC')\n",
    "\n",
    "        print('>> ' + ts_out + '\\n')\n",
    "\n",
    "        try:\n",
    "            neg_range = (sl[i].index(ts_out) + 1, sl[i].index(ts_out) + len(ts_out)) # negated place\n",
    "        except:\n",
    "            neg_range = (0, len(sl))\n",
    "        \n",
    "        print(neg_range)\n",
    "\n",
    "        for idx in df1.index:\n",
    "            if df1['sent_id'][idx] == i+1 and df1['sent_loc'][idx] in range(neg_range[0], neg_range[1]):\n",
    "                df1['negation'][idx] = 1\n",
    "                \n",
    "    except: # need to debug why very few cases don't work\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preserve the longest strings/concepts\n",
    "df_s = df1\n",
    "df_s['start'] = df_s['start'].astype(int)\n",
    "df_s['len'] = df_s['original'].str.len()\n",
    "df_s = df_s.sort_values('len', ascending=False)\n",
    "df_s = df_s.drop_duplicates(['sent_id', 'start'], keep='first')\n",
    "df_s = df_s.drop_duplicates(['sent_id', 'end'], keep='first')\n",
    "df_s = df_s.sort_values('start', ascending=True)\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_ss = df_s[(df_s.sent_id != 0) & (df_s.section != '')]\n",
    "\n",
    "def print_out_result(df):\n",
    "    for s in set(df['section'].values):\n",
    "        if s != '':\n",
    "            subset = df[df['section'] == s][['preferred', 'negation']]\n",
    "            subset['preferred'] = np.where(subset['negation'] == 1, subset['preferred'] + '(-)', subset['preferred'] + '(+)')\n",
    "            print '--- ' + s + ' ---\\n' + ', '.join(subset['preferred'])\n",
    "    \n",
    "print_out_result(df_ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
